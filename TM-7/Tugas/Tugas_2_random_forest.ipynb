{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "from sklearn import metrics\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import StratifiedKFold,cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,mean_absolute_error,mean_squared_error,r2_score,make_scorer\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base class for the random forest algorithm\n",
    "class RandomForest(ABC):\n",
    "    #initializer\n",
    "    def __init__(self,n_trees=100):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees   = []\n",
    "    #private function to make bootstrap samples\n",
    "    def __make_bootstraps(self,data):\n",
    "        #initialize output dictionary & unique value count\n",
    "        dc   = {}\n",
    "        unip = 0\n",
    "        #get sample size\n",
    "        b_size = data.shape[0]\n",
    "        #get list of row indexes\n",
    "        idx = [i for i in range(b_size)]\n",
    "        #loop through the required number of bootstraps\n",
    "        for b in range(self.n_trees):\n",
    "            #obtain boostrap samples with replacement\n",
    "            sidx   = np.random.choice(idx,replace=True,size=b_size)\n",
    "            b_samp = data[sidx,:]\n",
    "            #compute number of unique values contained in the bootstrap sample\n",
    "            unip  += len(set(sidx))\n",
    "            #obtain out-of-bag samples for the current b\n",
    "            oidx   = list(set(idx) - set(sidx))\n",
    "            o_samp = np.array([])\n",
    "            if oidx:\n",
    "                o_samp = data[oidx,:]\n",
    "            #store results\n",
    "            dc['boot_'+str(b)] = {'boot':b_samp,'test':o_samp}\n",
    "        #return the bootstrap results\n",
    "        return(dc)\n",
    "\n",
    "    #public function to return model parameters\n",
    "    def get_params(self, deep = False):\n",
    "        return {'n_trees':self.n_trees}\n",
    "\n",
    "    #protected function to obtain the right decision tree\n",
    "    @abstractmethod\n",
    "    def _make_tree_model(self):\n",
    "        pass\n",
    "    \n",
    "    #protected function to train the ensemble\n",
    "    def _train(self,X_train,y_train):\n",
    "        #package the input data\n",
    "        training_data = np.concatenate((X_train,y_train.reshape(-1,1)),axis=1)\n",
    "        #make bootstrap samples\n",
    "        dcBoot = self.__make_bootstraps(training_data)\n",
    "        #iterate through each bootstrap sample & fit a model ##\n",
    "        tree_m = self._make_tree_model()\n",
    "        dcOob    = {}\n",
    "        for b in dcBoot:\n",
    "            #make a clone of the model\n",
    "            model = clone(tree_m)\n",
    "            #fit a decision tree model to the current sample\n",
    "            model.fit(dcBoot[b]['boot'][:,:-1],dcBoot[b]['boot'][:,-1].reshape(-1, 1))\n",
    "            #append the fitted model\n",
    "            self.trees.append(model)\n",
    "            #store the out-of-bag test set for the current bootstrap\n",
    "            if dcBoot[b]['test'].size:\n",
    "                dcOob[b] = dcBoot[b]['test']\n",
    "            else:\n",
    "                dcOob[b] = np.array([])\n",
    "        #return the oob data set\n",
    "        return(dcOob)\n",
    "      \n",
    "    #protected function to predict from the ensemble\n",
    "    def _predict(self,X):\n",
    "        #check we've fit the ensemble\n",
    "        if not self.trees:\n",
    "            print('You must train the ensemble before making predictions!')\n",
    "            return(None)\n",
    "        #loop through each fitted model\n",
    "        predictions = []\n",
    "        for m in self.trees:\n",
    "            #make predictions on the input X\n",
    "            yp = m.predict(X)\n",
    "            #append predictions to storage list\n",
    "            predictions.append(yp.reshape(-1,1))\n",
    "        #compute the ensemble prediction\n",
    "        ypred = np.mean(np.concatenate(predictions,axis=1),axis=1)\n",
    "        #return the prediction\n",
    "        return(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for random forest classifier\n",
    "class RandomForestClassifierCustom(RandomForest):\n",
    "   #initializer\n",
    "    def __init__(self,n_trees=100,max_depth: int=None,min_samples_split: int=2,criterion: str='gini',class_weights='balanced'):\n",
    "        super().__init__(n_trees)\n",
    "        self.max_depth             = max_depth\n",
    "        self.min_samples_split     = min_samples_split\n",
    "        self.criterion             = criterion\n",
    "        self.class_weights         = class_weights\n",
    "\n",
    "    #protected function to obtain the right decision tree\n",
    "    def _make_tree_model(self):\n",
    "        return(DecisionTreeClassifier(max_depth             = self.max_depth,\n",
    "                                      min_samples_split     = self.min_samples_split,\n",
    "                                      criterion             = self.criterion,\n",
    "                                      class_weight          = self.class_weights))\n",
    "\n",
    "    #public function to return model parameters\n",
    "    def get_params(self, deep = False):\n",
    "        return {'n_trees':self.n_trees,\n",
    "                'max_depth':self.max_depth,\n",
    "                'min_samples_split':self.min_samples_split,\n",
    "                'criterion':self.criterion,\n",
    "                'class_weights':self.class_weights}\n",
    "\n",
    "    #train the ensemble\n",
    "    def fit(self,X_train,y_train,print_metrics=False):\n",
    "        #call the protected training method\n",
    "        dcOob = self._train(X_train,y_train)\n",
    "        #if selected, compute the standard errors and print them\n",
    "        if print_metrics:\n",
    "            #initialise metric arrays\n",
    "            accs = np.array([])\n",
    "            pres = np.array([])\n",
    "            recs = np.array([])\n",
    "            #loop through each bootstrap sample\n",
    "            for b,m in zip(dcOob,self.trees):\n",
    "                #compute the predictions on the out-of-bag test set & compute metrics\n",
    "                if dcOob[b].size:\n",
    "                    yp  = m.predict(dcOob[b][:,:-1])\n",
    "                    acc = accuracy_score(dcOob[b][:,-1],yp)\n",
    "                    pre = precision_score(dcOob[b][:,-1],yp,average='weighted')\n",
    "                    rec = recall_score(dcOob[b][:,-1],yp,average='weighted')\n",
    "                    #store the error metrics\n",
    "                    accs = np.concatenate((accs,acc.flatten()))\n",
    "                    pres = np.concatenate((pres,pre.flatten()))\n",
    "                    recs = np.concatenate((recs,rec.flatten()))\n",
    "            #print standard errors\n",
    "            print(\"Standard error in accuracy: %.2f\" % np.std(accs))\n",
    "            print(\"Standard error in precision: %.2f\" % np.std(pres))\n",
    "            print(\"Standard error in recall: %.2f\" % np.std(recs))\n",
    "\n",
    "    #predict from the ensemble\n",
    "    def predict(self,X):\n",
    "        #call the protected prediction method\n",
    "        ypred = self._predict(X)\n",
    "        #convert the results into integer values & return\n",
    "        return(np.round(ypred).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X:  (178, 13)\n",
      "Dimensions of y:  (178,)\n",
      "Classes in the label:  [0 1 2]\n",
      "highly correlated input features:  total_phenols  &  ['flavanoids']\n",
      "highly correlated input features:  flavanoids  &  ['total_phenols']\n",
      "Mean Accuracy: 0.94\n",
      "Mean Precision: 0.95\n",
      "Mean Recall: 0.94\n"
     ]
    }
   ],
   "source": [
    "#load the wine dataset\n",
    "dfX,sY = load_wine(return_X_y=True, as_frame=True)\n",
    "\n",
    "#check the dimensions of these data\n",
    "print('Dimensions of X: ',dfX.shape)\n",
    "print('Dimensions of y: ',sY.shape)\n",
    "\n",
    "#what unique classes exist in the label variable?\n",
    "print('Classes in the label: ',sY.unique())\n",
    "\n",
    "dfCorr  = dfX.corr()\n",
    "\n",
    "#convert all correlations to positive values\n",
    "dfCorr = dfCorr.abs()\n",
    "\n",
    "#loop through rows\n",
    "for index, sRow in dfCorr.iterrows():\n",
    "    #get the valid entries\n",
    "    sCorrs = sRow[sRow.index != index]\n",
    "    sCorrs = sCorrs[sCorrs > 0.8]\n",
    "    #print out results\n",
    "    if not sCorrs.empty:\n",
    "        print('highly correlated input features: ',index,' & ',sCorrs.index.values)\n",
    "\n",
    "#create a random forest with balance class weights enabled\n",
    "rfcC = RandomForestClassifierCustom(class_weights='balanced')\n",
    "\n",
    "## train the ensemble & view estimates for prediction error ##\n",
    "rfcC.fit(dfX.values,sY.values,print_metrics=False)\n",
    "\n",
    "## use k fold cross validation to measure performance ##\n",
    "scoring_metrics = {'accuracy': make_scorer(accuracy_score),\n",
    "                   'precision': make_scorer(precision_score, average='weighted'),\n",
    "                   'recall': make_scorer(recall_score, average='weighted')}\n",
    "dcScores        = cross_validate(rfcC,dfX.values,sY.values,cv=StratifiedKFold(10),scoring=scoring_metrics)\n",
    "print('Mean Accuracy: %.2f' % np.mean(dcScores['test_accuracy']))\n",
    "print('Mean Precision: %.2f' % np.mean(dcScores['test_precision']))\n",
    "print('Mean Recall: %.2f' % np.mean(dcScores['test_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
